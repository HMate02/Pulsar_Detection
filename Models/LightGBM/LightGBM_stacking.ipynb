{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9445,
     "status": "ok",
     "timestamp": 1724944527129,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "dZuzg1K9XZIQ",
    "outputId": "a0ed814f-4b88-4935-d240-389222a7a143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (1.13.2)\n",
      "Requirement already satisfied: colorlog in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (2.0.32)\n",
      "Requirement already satisfied: tqdm in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (8.2.0)\n",
      "Requirement already satisfied: importlib-resources in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Importing the models\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Importing sampling methods\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "TRIALS = 5\n",
    "MODEL_NUM = 5\n",
    "NUM_ITER = 1000\n",
    "SEED = 2024\n",
    "SEED_OBJ = SEED\n",
    "SAMPLER = TPESampler(seed=SEED)\n",
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fjlkGPDeaKAI"
   },
   "outputs": [],
   "source": [
    "htru2_data = pd.read_csv('https://raw.githubusercontent.com/szbela87/ml_22_elteik/main/data/HTRU_2.csv', header=None)\n",
    "htru2_data.columns = ['mean_ip', 'std_ip', 'excess_kurt_ip', 'skewness_ip', 'mean_DMSNR', 'std_DMSNR', 'excess_kurt_DMSNR', 'skewness_DMSNR', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1724944527131,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "NABmAOA8aOMz",
    "outputId": "0442f4ae-8597-4666-f866-cefba7e05d34"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_ip</th>\n",
       "      <th>std_ip</th>\n",
       "      <th>excess_kurt_ip</th>\n",
       "      <th>skewness_ip</th>\n",
       "      <th>mean_DMSNR</th>\n",
       "      <th>std_DMSNR</th>\n",
       "      <th>excess_kurt_DMSNR</th>\n",
       "      <th>skewness_DMSNR</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>136.429688</td>\n",
       "      <td>59.847421</td>\n",
       "      <td>-0.187846</td>\n",
       "      <td>-0.738123</td>\n",
       "      <td>1.296823</td>\n",
       "      <td>12.166062</td>\n",
       "      <td>15.450260</td>\n",
       "      <td>285.931022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17894</th>\n",
       "      <td>122.554688</td>\n",
       "      <td>49.485605</td>\n",
       "      <td>0.127978</td>\n",
       "      <td>0.323061</td>\n",
       "      <td>16.409699</td>\n",
       "      <td>44.626893</td>\n",
       "      <td>2.945244</td>\n",
       "      <td>8.297092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17895</th>\n",
       "      <td>119.335938</td>\n",
       "      <td>59.935939</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>-0.743025</td>\n",
       "      <td>21.430602</td>\n",
       "      <td>58.872000</td>\n",
       "      <td>2.499517</td>\n",
       "      <td>4.595173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>114.507812</td>\n",
       "      <td>53.902400</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>1.946488</td>\n",
       "      <td>13.381731</td>\n",
       "      <td>10.007967</td>\n",
       "      <td>134.238910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17897</th>\n",
       "      <td>57.062500</td>\n",
       "      <td>85.797340</td>\n",
       "      <td>1.406391</td>\n",
       "      <td>0.089520</td>\n",
       "      <td>188.306020</td>\n",
       "      <td>64.712562</td>\n",
       "      <td>-1.597527</td>\n",
       "      <td>1.429475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17898 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean_ip     std_ip  excess_kurt_ip  skewness_ip  mean_DMSNR  \\\n",
       "0      140.562500  55.683782       -0.234571    -0.699648    3.199833   \n",
       "1      102.507812  58.882430        0.465318    -0.515088    1.677258   \n",
       "2      103.015625  39.341649        0.323328     1.051164    3.121237   \n",
       "3      136.750000  57.178449       -0.068415    -0.636238    3.642977   \n",
       "4       88.726562  40.672225        0.600866     1.123492    1.178930   \n",
       "...           ...        ...             ...          ...         ...   \n",
       "17893  136.429688  59.847421       -0.187846    -0.738123    1.296823   \n",
       "17894  122.554688  49.485605        0.127978     0.323061   16.409699   \n",
       "17895  119.335938  59.935939        0.159363    -0.743025   21.430602   \n",
       "17896  114.507812  53.902400        0.201161    -0.024789    1.946488   \n",
       "17897   57.062500  85.797340        1.406391     0.089520  188.306020   \n",
       "\n",
       "       std_DMSNR  excess_kurt_DMSNR  skewness_DMSNR  class  \n",
       "0      19.110426           7.975532       74.242225      0  \n",
       "1      14.860146          10.576487      127.393580      0  \n",
       "2      21.744669           7.735822       63.171909      0  \n",
       "3      20.959280           6.896499       53.593661      0  \n",
       "4      11.468720          14.269573      252.567306      0  \n",
       "...          ...                ...             ...    ...  \n",
       "17893  12.166062          15.450260      285.931022      0  \n",
       "17894  44.626893           2.945244        8.297092      0  \n",
       "17895  58.872000           2.499517        4.595173      0  \n",
       "17896  13.381731          10.007967      134.238910      0  \n",
       "17897  64.712562          -1.597527        1.429475      0  \n",
       "\n",
       "[17898 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1724944527131,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "ywq_yxppaRH6",
    "outputId": "745ceaa6-8d64-4cda-de0c-06f1f89b186c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_ip</th>\n",
       "      <th>std_ip</th>\n",
       "      <th>excess_kurt_ip</th>\n",
       "      <th>skewness_ip</th>\n",
       "      <th>mean_DMSNR</th>\n",
       "      <th>std_DMSNR</th>\n",
       "      <th>excess_kurt_DMSNR</th>\n",
       "      <th>skewness_DMSNR</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.079968</td>\n",
       "      <td>46.549532</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>1.770279</td>\n",
       "      <td>12.614400</td>\n",
       "      <td>26.326515</td>\n",
       "      <td>8.303556</td>\n",
       "      <td>104.857709</td>\n",
       "      <td>0.091574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.652935</td>\n",
       "      <td>6.843189</td>\n",
       "      <td>1.064040</td>\n",
       "      <td>6.167913</td>\n",
       "      <td>29.472897</td>\n",
       "      <td>19.470572</td>\n",
       "      <td>4.506092</td>\n",
       "      <td>106.514540</td>\n",
       "      <td>0.288432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.812500</td>\n",
       "      <td>24.772042</td>\n",
       "      <td>-1.876011</td>\n",
       "      <td>-1.791886</td>\n",
       "      <td>0.213211</td>\n",
       "      <td>7.370432</td>\n",
       "      <td>-3.139270</td>\n",
       "      <td>-1.976976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.929688</td>\n",
       "      <td>42.376018</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>-0.188572</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>14.437332</td>\n",
       "      <td>5.781506</td>\n",
       "      <td>34.960504</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>115.078125</td>\n",
       "      <td>46.947479</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.198710</td>\n",
       "      <td>2.801839</td>\n",
       "      <td>18.461316</td>\n",
       "      <td>8.433515</td>\n",
       "      <td>83.064556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.085938</td>\n",
       "      <td>51.023202</td>\n",
       "      <td>0.473325</td>\n",
       "      <td>0.927783</td>\n",
       "      <td>5.464256</td>\n",
       "      <td>28.428104</td>\n",
       "      <td>10.702959</td>\n",
       "      <td>139.309330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>192.617188</td>\n",
       "      <td>98.778911</td>\n",
       "      <td>8.069522</td>\n",
       "      <td>68.101622</td>\n",
       "      <td>223.392141</td>\n",
       "      <td>110.642211</td>\n",
       "      <td>34.539844</td>\n",
       "      <td>1191.000837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean_ip        std_ip  excess_kurt_ip   skewness_ip    mean_DMSNR  \\\n",
       "count  17898.000000  17898.000000    17898.000000  17898.000000  17898.000000   \n",
       "mean     111.079968     46.549532        0.477857      1.770279     12.614400   \n",
       "std       25.652935      6.843189        1.064040      6.167913     29.472897   \n",
       "min        5.812500     24.772042       -1.876011     -1.791886      0.213211   \n",
       "25%      100.929688     42.376018        0.027098     -0.188572      1.923077   \n",
       "50%      115.078125     46.947479        0.223240      0.198710      2.801839   \n",
       "75%      127.085938     51.023202        0.473325      0.927783      5.464256   \n",
       "max      192.617188     98.778911        8.069522     68.101622    223.392141   \n",
       "\n",
       "          std_DMSNR  excess_kurt_DMSNR  skewness_DMSNR         class  \n",
       "count  17898.000000       17898.000000    17898.000000  17898.000000  \n",
       "mean      26.326515           8.303556      104.857709      0.091574  \n",
       "std       19.470572           4.506092      106.514540      0.288432  \n",
       "min        7.370432          -3.139270       -1.976976      0.000000  \n",
       "25%       14.437332           5.781506       34.960504      0.000000  \n",
       "50%       18.461316           8.433515       83.064556      0.000000  \n",
       "75%       28.428104          10.702959      139.309330      0.000000  \n",
       "max      110.642211          34.539844     1191.000837      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1724944527131,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "r4ZHzC9laS_k",
    "outputId": "89ae2d1d-6f0c-44dd-8c82-39f10a85151d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17898, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1724944527131,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "41bHGH-oaVYr",
    "outputId": "4ae09cc4-9cd5-4748-bbf2-649564b4caf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    16259\n",
       "1     1639\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1724944527432,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "aEIJK01YaXIK",
    "outputId": "7fcfc14b-1e0f-4282-b372-434211f62d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16108, 8), (16108,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data (80/10/10% - train/validation/test)\n",
    "\n",
    "X = htru2_data.drop(\"class\", axis=1)\n",
    "y = htru2_data[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1724944527432,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "E4Tt7sSwaaDM",
    "outputId": "b29fd36f-20ca-4802-87f5-3df66a24f9ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14318, 8), (14318,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1/0.9, random_state=SEED)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1724944527432,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "7m10HLxqa2s8",
    "outputId": "d31af5fb-eec4-4a46-9952-088bc7db98f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14318, 8), (14318,), (1790, 8), (1790,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1724944527432,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "nU8hUz4la4Yc",
    "outputId": "6c81c44b-9cd0-4ea2-beb0-0591f68b5708"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(class\n",
       " 0    13014\n",
       " 1     1304\n",
       " Name: count, dtype: int64,\n",
       " class\n",
       " 0    1627\n",
       " 1     163\n",
       " Name: count, dtype: int64,\n",
       " class\n",
       " 0    1618\n",
       " 1     172\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(), y_val.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-eVMmUye-39"
   },
   "source": [
    "# LightGBM StackingClassifier fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226529,
     "status": "ok",
     "timestamp": 1724944971189,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "pECwAqsbm1k3",
    "outputId": "5015a5b7-289e-4d1c-bc23-1c23cb499d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9849\n",
      "AUC: 0.9449\n",
      "F1-score: 0.9194\n",
      "Precision: 0.9448\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1609    9]\n",
      " [  18  154]]\n",
      "Runtime: 13786.799222428817\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9832\n",
      "AUC: 0.9440\n",
      "F1-score: 0.9112\n",
      "Precision: 0.9277\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1606   12]\n",
      " [  18  154]]\n",
      "Runtime: 8649.777214934118\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9855\n",
      "AUC: 0.9452\n",
      "F1-score: 0.9222\n",
      "Precision: 0.9506\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1610    8]\n",
      " [  18  154]]\n",
      "Runtime: 12314.726405731868\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9844\n",
      "AUC: 0.9420\n",
      "F1-score: 0.9162\n",
      "Precision: 0.9444\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1609    9]\n",
      " [  19  153]]\n",
      "Runtime: 15022.434128265362\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9855\n",
      "AUC: 0.9426\n",
      "F1-score: 0.9217\n",
      "Precision: 0.9563\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1611    7]\n",
      " [  19  153]]\n",
      "Runtime: 13405.079136505723\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9445\n",
      "Accuracy: 0.9832\n",
      "AUC: 0.9440\n",
      "F1-score: 0.9112\n",
      "Precision: 0.9277\n",
      "Recall score: 0.8953\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9847\n",
      "Average AUC: 0.9437\n",
      "Average f1-score: 0.9181\n",
      "Average precision: 0.9448\n",
      "Average recall score: 0.8930\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # LightGBM parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"num_leaves_{j}\" : trial.suggest_int(f\"num_leaves_{j}\", 20, 50),\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.2),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 100, 1000),\n",
    "          f\"min_split_gain_{j}\" : trial.suggest_uniform(f\"min_split_gain_{j}\", 0.0, 0.5),\n",
    "          f\"min_child_weight_{j}\" : trial.suggest_loguniform(f\"min_child_weight_{j}\", 1e-3, 1.0),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1.0),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1.0),\n",
    "          f\"lambda_{j}\" : trial.suggest_loguniform(f\"lambda_{j}\", 1e-8, 10.0)\n",
    "      }\n",
    "\n",
    "      model_params = {\n",
    "          \"num_leaves\" : params[f\"num_leaves_{j}\"],\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"],\n",
    "          \"min_split_gain\" : params[f\"min_split_gain_{j}\"],\n",
    "          \"min_child_weight\" : params[f\"min_child_weight_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"lambda\" : params[f\"lambda_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"lgb{j}\", lgb.LGBMClassifier(random_state=SEED_OBJ, verbosity=-1, **model_params)))\n",
    "\n",
    "    # Parameters of LogisticRegression\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_train, y_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  lgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  lgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = lgb_study.best_trial.params\n",
    "\n",
    "  # LightGBM StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"num_leaves\" : best_params[f\"num_leaves_{k}\"],\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"],\n",
    "        \"min_split_gain\" : best_params[f\"min_split_gain_{k}\"],\n",
    "        \"min_child_weight\" : best_params[f\"min_child_weight_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"lambda\" : best_params[f\"lambda_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"lgb{k}\", lgb.LGBMClassifier(random_state=SEED, verbosity=-1, **model_params)))\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned LightGBM StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < lgb_study.best_trial.value:\n",
    "    max_val_auc_score = lgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Best validation auc scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_lightgbm_without_smote.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sj2-vWYe-WF"
   },
   "source": [
    "# LightGBM StackingClassifier with SMOTE and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325882,
     "status": "ok",
     "timestamp": 1724945297056,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "iQ8cMqMifMGX",
    "outputId": "e6de5240-1d30-4207-dab3-f276f8d83b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9721\n",
      "AUC: 0.9508\n",
      "F1-score: 0.8641\n",
      "Precision: 0.8112\n",
      "Recall score: 0.9244\n",
      "Confusion matrix:\n",
      " [[1581   37]\n",
      " [  13  159]]\n",
      "Runtime: 24239.443389546126\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9704\n",
      "AUC: 0.9498\n",
      "F1-score: 0.8571\n",
      "Precision: 0.7990\n",
      "Recall score: 0.9244\n",
      "Confusion matrix:\n",
      " [[1578   40]\n",
      " [  13  159]]\n",
      "Runtime: 19142.18049670197\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9704\n",
      "AUC: 0.9498\n",
      "F1-score: 0.8571\n",
      "Precision: 0.7990\n",
      "Recall score: 0.9244\n",
      "Confusion matrix:\n",
      " [[1578   40]\n",
      " [  13  159]]\n",
      "Runtime: 23106.19133951422\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9732\n",
      "AUC: 0.9436\n",
      "F1-score: 0.8667\n",
      "Precision: 0.8298\n",
      "Recall score: 0.9070\n",
      "Confusion matrix:\n",
      " [[1586   32]\n",
      " [  16  156]]\n",
      "Runtime: 19219.90137112001\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9721\n",
      "AUC: 0.9508\n",
      "F1-score: 0.8641\n",
      "Precision: 0.8112\n",
      "Recall score: 0.9244\n",
      "Confusion matrix:\n",
      " [[1581   37]\n",
      " [  13  159]]\n",
      "Runtime: 22853.536697069183\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9509\n",
      "Accuracy: 0.9732\n",
      "AUC: 0.9436\n",
      "F1-score: 0.8667\n",
      "Precision: 0.8298\n",
      "Recall score: 0.9070\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9716\n",
      "Average AUC: 0.9490\n",
      "Average f1-score: 0.8618\n",
      "Average precision: 0.8100\n",
      "Average recall score: 0.9209\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # LightGBM parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"num_leaves_{j}\" : trial.suggest_int(f\"num_leaves_{j}\", 20, 50),\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.2),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 100, 1000),\n",
    "          f\"min_split_gain_{j}\" : trial.suggest_uniform(f\"min_split_gain_{j}\", 0.0, 0.5),\n",
    "          f\"min_child_weight_{j}\" : trial.suggest_loguniform(f\"min_child_weight_{j}\", 1e-3, 1.0),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1.0),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1.0),\n",
    "          f\"lambda_{j}\" : trial.suggest_loguniform(f\"lambda_{j}\", 1e-8, 10.0)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"num_leaves\" : params[f\"num_leaves_{j}\"],\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"],\n",
    "          \"min_split_gain\" : params[f\"min_split_gain_{j}\"],\n",
    "          \"min_child_weight\" : params[f\"min_child_weight_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"lambda\" : params[f\"lambda_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"lgb{j}\", lgb.LGBMClassifier(random_state=SEED_OBJ, verbosity=-1, **model_params)))\n",
    "\n",
    "    # SMOTE parameter\n",
    "\n",
    "    smote_kn = trial.suggest_int(\"k_neighbors\", 10, 230)\n",
    "\n",
    "    smote_train = SMOTE(sampling_strategy=\"minority\", k_neighbors=smote_kn, random_state=SEED_OBJ)\n",
    "    X_smote_train, y_smote_train = smote_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_smote_train, y_smote_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  lgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  lgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = lgb_study.best_trial.params\n",
    "\n",
    "  # LightGBM StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"num_leaves\" : best_params[f\"num_leaves_{k}\"],\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"],\n",
    "        \"min_split_gain\" : best_params[f\"min_split_gain_{k}\"],\n",
    "        \"min_child_weight\" : best_params[f\"min_child_weight_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"lambda\" : best_params[f\"lambda_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"lgb{k}\", lgb.LGBMClassifier(random_state=SEED, verbosity=-1, **model_params)))\n",
    "\n",
    "  # SMOTE best parameter\n",
    "\n",
    "  smote_kn = best_params[\"k_neighbors\"]\n",
    "\n",
    "  smote_train = SMOTE(sampling_strategy=\"minority\", k_neighbors=smote_kn, random_state=SEED)\n",
    "  X_smote_train, y_smote_train = smote_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned LightGBM StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_smote_train, y_smote_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < lgb_study.best_trial.value:\n",
    "    max_val_auc_score = lgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Best validation auc scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_lightgbm_with_smote.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOJ8SEaUfOet"
   },
   "source": [
    "# LightGBM StackingClassifier with Oversampling (ADASYN) and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343197,
     "status": "ok",
     "timestamp": 1724945640220,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "bqGqxJEzfYp8",
    "outputId": "764ef8ff-3838-4e45-b4e7-db3bb026c354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9547\n",
      "AUC: 0.9464\n",
      "F1-score: 0.7990\n",
      "Precision: 0.6970\n",
      "Recall score: 0.9360\n",
      "Confusion matrix:\n",
      " [[1548   70]\n",
      " [  11  161]]\n",
      "Runtime: 21878.201284334995\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9553\n",
      "AUC: 0.9441\n",
      "F1-score: 0.8000\n",
      "Precision: 0.7018\n",
      "Recall score: 0.9302\n",
      "Confusion matrix:\n",
      " [[1550   68]\n",
      " [  12  160]]\n",
      "Runtime: 30560.640570339747\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9570\n",
      "AUC: 0.9398\n",
      "F1-score: 0.8041\n",
      "Precision: 0.7149\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1555   63]\n",
      " [  14  158]]\n",
      "Runtime: 22543.926722519565\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9553\n",
      "AUC: 0.9467\n",
      "F1-score: 0.8010\n",
      "Precision: 0.7000\n",
      "Recall score: 0.9360\n",
      "Confusion matrix:\n",
      " [[1549   69]\n",
      " [  11  161]]\n",
      "Runtime: 23914.558225593995\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9570\n",
      "AUC: 0.9528\n",
      "F1-score: 0.8089\n",
      "Precision: 0.7056\n",
      "Recall score: 0.9477\n",
      "Confusion matrix:\n",
      " [[1550   68]\n",
      " [   9  163]]\n",
      "Runtime: 20959.33299438283\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9478\n",
      "Accuracy: 0.9547\n",
      "AUC: 0.9464\n",
      "F1-score: 0.7990\n",
      "Precision: 0.6970\n",
      "Recall score: 0.9360\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9559\n",
      "Average AUC: 0.9460\n",
      "Average f1-score: 0.8026\n",
      "Average precision: 0.7039\n",
      "Average recall score: 0.9337\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # LightGBM parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"num_leaves_{j}\" : trial.suggest_int(f\"num_leaves_{j}\", 20, 50),\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.2),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 100, 1000),\n",
    "          f\"min_split_gain_{j}\" : trial.suggest_uniform(f\"min_split_gain_{j}\", 0.0, 0.5),\n",
    "          f\"min_child_weight_{j}\" : trial.suggest_loguniform(f\"min_child_weight_{j}\", 1e-3, 1.0),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1.0),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1.0),\n",
    "          f\"lambda_{j}\" : trial.suggest_loguniform(f\"lambda_{j}\", 1e-8, 10.0)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"num_leaves\" : params[f\"num_leaves_{j}\"],\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"],\n",
    "          \"min_split_gain\" : params[f\"min_split_gain_{j}\"],\n",
    "          \"min_child_weight\" : params[f\"min_child_weight_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"lambda\" : params[f\"lambda_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"lgb{j}\", lgb.LGBMClassifier(random_state=SEED_OBJ, verbosity=-1, **model_params)))\n",
    "\n",
    "    # ADASYN parameter\n",
    "\n",
    "    adasyn_nn = trial.suggest_int(\"n_neighbors\", 10, 230)\n",
    "\n",
    "    adasyn_train = ADASYN(sampling_strategy=\"minority\", n_neighbors=adasyn_nn, random_state=SEED_OBJ)\n",
    "    X_adasyn_train, y_adasyn_train = adasyn_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_adasyn_train, y_adasyn_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  lgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  lgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = lgb_study.best_trial.params\n",
    "\n",
    "  # LightGBM StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"num_leaves\" : best_params[f\"num_leaves_{k}\"],\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"],\n",
    "        \"min_split_gain\" : best_params[f\"min_split_gain_{k}\"],\n",
    "        \"min_child_weight\" : best_params[f\"min_child_weight_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"lambda\" : best_params[f\"lambda_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"lgb{k}\", lgb.LGBMClassifier(random_state=SEED, verbosity=-1, **model_params)))\n",
    "\n",
    "  # ADASYN best parameter\n",
    "\n",
    "  adasyn_nn = best_params[\"n_neighbors\"]\n",
    "\n",
    "  adasyn_train = ADASYN(sampling_strategy=\"minority\", n_neighbors=adasyn_nn, random_state=SEED)\n",
    "  X_adasyn_train, y_adasyn_train = adasyn_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned LightGBM StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_adasyn_train, y_adasyn_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < lgb_study.best_trial.value:\n",
    "    max_val_auc_score = lgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Test scores with best validation AUC score\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_lightgbm_with_adasyn.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGAez5YSmOpS"
   },
   "source": [
    "# LightGBM StackingClassifier with Undersampling (TomekLinks) and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229811,
     "status": "ok",
     "timestamp": 1724945870019,
     "user": {
      "displayName": "Máté Horváth",
      "userId": "15696021487141530649"
     },
     "user_tz": -120
    },
    "id": "8rBYU-FnmRtP",
    "outputId": "0e978ad2-7a0f-46b5-a77b-9e629c43fcbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9827\n",
      "AUC: 0.9411\n",
      "F1-score: 0.9080\n",
      "Precision: 0.9273\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1606   12]\n",
      " [  19  153]]\n",
      "Runtime: 10010.837662018836\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9827\n",
      "AUC: 0.9411\n",
      "F1-score: 0.9080\n",
      "Precision: 0.9273\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1606   12]\n",
      " [  19  153]]\n",
      "Runtime: 12514.837733316235\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9816\n",
      "AUC: 0.9404\n",
      "F1-score: 0.9027\n",
      "Precision: 0.9162\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1604   14]\n",
      " [  19  153]]\n",
      "Runtime: 10312.28130711522\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9827\n",
      "AUC: 0.9385\n",
      "F1-score: 0.9075\n",
      "Precision: 0.9325\n",
      "Recall score: 0.8837\n",
      "Confusion matrix:\n",
      " [[1607   11]\n",
      " [  20  152]]\n",
      "Runtime: 11510.53915929515\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9821\n",
      "AUC: 0.9382\n",
      "F1-score: 0.9048\n",
      "Precision: 0.9268\n",
      "Recall score: 0.8837\n",
      "Confusion matrix:\n",
      " [[1606   12]\n",
      " [  20  152]]\n",
      "Runtime: 9926.112030077726\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9414\n",
      "Accuracy: 0.9821\n",
      "AUC: 0.9382\n",
      "F1-score: 0.9048\n",
      "Precision: 0.9268\n",
      "Recall score: 0.8837\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9823\n",
      "Average AUC: 0.9398\n",
      "Average f1-score: 0.9062\n",
      "Average precision: 0.9260\n",
      "Average recall score: 0.8872\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # LightGBM parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"num_leaves_{j}\" : trial.suggest_int(f\"num_leaves_{j}\", 20, 50),\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.2),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 100, 1000),\n",
    "          f\"min_split_gain_{j}\" : trial.suggest_uniform(f\"min_split_gain_{j}\", 0.0, 0.5),\n",
    "          f\"min_child_weight_{j}\" : trial.suggest_loguniform(f\"min_child_weight_{j}\", 1e-3, 1.0),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1.0),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1.0),\n",
    "          f\"lambda_{j}\" : trial.suggest_loguniform(f\"lambda_{j}\", 1e-8, 10.0)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"num_leaves\" : params[f\"num_leaves_{j}\"],\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"],\n",
    "          \"min_split_gain\" : params[f\"min_split_gain_{j}\"],\n",
    "          \"min_child_weight\" : params[f\"min_child_weight_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"lambda\" : params[f\"lambda_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"lgb{j}\", lgb.LGBMClassifier(random_state=SEED_OBJ, verbosity=-1, **model_params)))\n",
    "\n",
    "    # TomekLinks\n",
    "\n",
    "    tomek_train = TomekLinks(sampling_strategy=\"majority\")\n",
    "    X_tomek_train, y_tomek_train = tomek_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_tomek_train, y_tomek_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  lgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  lgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = lgb_study.best_trial.params\n",
    "\n",
    "  # LightGBM StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"num_leaves\" : best_params[f\"num_leaves_{k}\"],\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"],\n",
    "        \"min_split_gain\" : best_params[f\"min_split_gain_{k}\"],\n",
    "        \"min_child_weight\" : best_params[f\"min_child_weight_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"lambda\" : best_params[f\"lambda_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"lgb{k}\", lgb.LGBMClassifier(random_state=SEED, verbosity=-1, **model_params)))\n",
    "\n",
    "  # TomekLinks\n",
    "\n",
    "  tomek_train = TomekLinks(sampling_strategy=\"majority\")\n",
    "  X_tomek_train, y_tomek_train = tomek_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned LightGBM StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_tomek_train, y_tomek_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < lgb_study.best_trial.value:\n",
    "    max_val_auc_score = lgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Test scores with best validation AUC score\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_lightgbm_with_tomeklinks.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfruHYq51O9U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWI3eJuxUGry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF8D0sHjUGfq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

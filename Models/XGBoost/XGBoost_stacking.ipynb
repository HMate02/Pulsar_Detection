{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZuzg1K9XZIQ",
    "outputId": "bdab293e-e65f-4ee2-88b2-6ec3ed3ae8e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (1.13.2)\n",
      "Requirement already satisfied: colorlog in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (2.0.32)\n",
      "Requirement already satisfied: tqdm in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (8.2.0)\n",
      "Requirement already satisfied: importlib-resources in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/szekeres/anaconda3/envs/htru2/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Importing the models\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Importing sampling methods\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "TRIALS = 5\n",
    "MODEL_NUM = 5\n",
    "NUM_ITER = 100\n",
    "SEED = 2024\n",
    "SEED_OBJ = SEED\n",
    "SAMPLER = TPESampler(seed=SEED)\n",
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fjlkGPDeaKAI"
   },
   "outputs": [],
   "source": [
    "htru2_data = pd.read_csv('https://raw.githubusercontent.com/szbela87/ml_22_elteik/main/data/HTRU_2.csv', header=None)\n",
    "htru2_data.columns = ['mean_ip', 'std_ip', 'excess_kurt_ip', 'skewness_ip', 'mean_DMSNR', 'std_DMSNR', 'excess_kurt_DMSNR', 'skewness_DMSNR', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "NABmAOA8aOMz",
    "outputId": "31997286-4e79-4005-b521-17cc1b5ff7c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_ip</th>\n",
       "      <th>std_ip</th>\n",
       "      <th>excess_kurt_ip</th>\n",
       "      <th>skewness_ip</th>\n",
       "      <th>mean_DMSNR</th>\n",
       "      <th>std_DMSNR</th>\n",
       "      <th>excess_kurt_DMSNR</th>\n",
       "      <th>skewness_DMSNR</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>136.429688</td>\n",
       "      <td>59.847421</td>\n",
       "      <td>-0.187846</td>\n",
       "      <td>-0.738123</td>\n",
       "      <td>1.296823</td>\n",
       "      <td>12.166062</td>\n",
       "      <td>15.450260</td>\n",
       "      <td>285.931022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17894</th>\n",
       "      <td>122.554688</td>\n",
       "      <td>49.485605</td>\n",
       "      <td>0.127978</td>\n",
       "      <td>0.323061</td>\n",
       "      <td>16.409699</td>\n",
       "      <td>44.626893</td>\n",
       "      <td>2.945244</td>\n",
       "      <td>8.297092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17895</th>\n",
       "      <td>119.335938</td>\n",
       "      <td>59.935939</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>-0.743025</td>\n",
       "      <td>21.430602</td>\n",
       "      <td>58.872000</td>\n",
       "      <td>2.499517</td>\n",
       "      <td>4.595173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>114.507812</td>\n",
       "      <td>53.902400</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>1.946488</td>\n",
       "      <td>13.381731</td>\n",
       "      <td>10.007967</td>\n",
       "      <td>134.238910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17897</th>\n",
       "      <td>57.062500</td>\n",
       "      <td>85.797340</td>\n",
       "      <td>1.406391</td>\n",
       "      <td>0.089520</td>\n",
       "      <td>188.306020</td>\n",
       "      <td>64.712562</td>\n",
       "      <td>-1.597527</td>\n",
       "      <td>1.429475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17898 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean_ip     std_ip  excess_kurt_ip  skewness_ip  mean_DMSNR  \\\n",
       "0      140.562500  55.683782       -0.234571    -0.699648    3.199833   \n",
       "1      102.507812  58.882430        0.465318    -0.515088    1.677258   \n",
       "2      103.015625  39.341649        0.323328     1.051164    3.121237   \n",
       "3      136.750000  57.178449       -0.068415    -0.636238    3.642977   \n",
       "4       88.726562  40.672225        0.600866     1.123492    1.178930   \n",
       "...           ...        ...             ...          ...         ...   \n",
       "17893  136.429688  59.847421       -0.187846    -0.738123    1.296823   \n",
       "17894  122.554688  49.485605        0.127978     0.323061   16.409699   \n",
       "17895  119.335938  59.935939        0.159363    -0.743025   21.430602   \n",
       "17896  114.507812  53.902400        0.201161    -0.024789    1.946488   \n",
       "17897   57.062500  85.797340        1.406391     0.089520  188.306020   \n",
       "\n",
       "       std_DMSNR  excess_kurt_DMSNR  skewness_DMSNR  class  \n",
       "0      19.110426           7.975532       74.242225      0  \n",
       "1      14.860146          10.576487      127.393580      0  \n",
       "2      21.744669           7.735822       63.171909      0  \n",
       "3      20.959280           6.896499       53.593661      0  \n",
       "4      11.468720          14.269573      252.567306      0  \n",
       "...          ...                ...             ...    ...  \n",
       "17893  12.166062          15.450260      285.931022      0  \n",
       "17894  44.626893           2.945244        8.297092      0  \n",
       "17895  58.872000           2.499517        4.595173      0  \n",
       "17896  13.381731          10.007967      134.238910      0  \n",
       "17897  64.712562          -1.597527        1.429475      0  \n",
       "\n",
       "[17898 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ywq_yxppaRH6",
    "outputId": "6d3c496e-2cff-4aee-ffbc-272fc5373bc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_ip</th>\n",
       "      <th>std_ip</th>\n",
       "      <th>excess_kurt_ip</th>\n",
       "      <th>skewness_ip</th>\n",
       "      <th>mean_DMSNR</th>\n",
       "      <th>std_DMSNR</th>\n",
       "      <th>excess_kurt_DMSNR</th>\n",
       "      <th>skewness_DMSNR</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.079968</td>\n",
       "      <td>46.549532</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>1.770279</td>\n",
       "      <td>12.614400</td>\n",
       "      <td>26.326515</td>\n",
       "      <td>8.303556</td>\n",
       "      <td>104.857709</td>\n",
       "      <td>0.091574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.652935</td>\n",
       "      <td>6.843189</td>\n",
       "      <td>1.064040</td>\n",
       "      <td>6.167913</td>\n",
       "      <td>29.472897</td>\n",
       "      <td>19.470572</td>\n",
       "      <td>4.506092</td>\n",
       "      <td>106.514540</td>\n",
       "      <td>0.288432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.812500</td>\n",
       "      <td>24.772042</td>\n",
       "      <td>-1.876011</td>\n",
       "      <td>-1.791886</td>\n",
       "      <td>0.213211</td>\n",
       "      <td>7.370432</td>\n",
       "      <td>-3.139270</td>\n",
       "      <td>-1.976976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.929688</td>\n",
       "      <td>42.376018</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>-0.188572</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>14.437332</td>\n",
       "      <td>5.781506</td>\n",
       "      <td>34.960504</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>115.078125</td>\n",
       "      <td>46.947479</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.198710</td>\n",
       "      <td>2.801839</td>\n",
       "      <td>18.461316</td>\n",
       "      <td>8.433515</td>\n",
       "      <td>83.064556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.085938</td>\n",
       "      <td>51.023202</td>\n",
       "      <td>0.473325</td>\n",
       "      <td>0.927783</td>\n",
       "      <td>5.464256</td>\n",
       "      <td>28.428104</td>\n",
       "      <td>10.702959</td>\n",
       "      <td>139.309330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>192.617188</td>\n",
       "      <td>98.778911</td>\n",
       "      <td>8.069522</td>\n",
       "      <td>68.101622</td>\n",
       "      <td>223.392141</td>\n",
       "      <td>110.642211</td>\n",
       "      <td>34.539844</td>\n",
       "      <td>1191.000837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean_ip        std_ip  excess_kurt_ip   skewness_ip    mean_DMSNR  \\\n",
       "count  17898.000000  17898.000000    17898.000000  17898.000000  17898.000000   \n",
       "mean     111.079968     46.549532        0.477857      1.770279     12.614400   \n",
       "std       25.652935      6.843189        1.064040      6.167913     29.472897   \n",
       "min        5.812500     24.772042       -1.876011     -1.791886      0.213211   \n",
       "25%      100.929688     42.376018        0.027098     -0.188572      1.923077   \n",
       "50%      115.078125     46.947479        0.223240      0.198710      2.801839   \n",
       "75%      127.085938     51.023202        0.473325      0.927783      5.464256   \n",
       "max      192.617188     98.778911        8.069522     68.101622    223.392141   \n",
       "\n",
       "          std_DMSNR  excess_kurt_DMSNR  skewness_DMSNR         class  \n",
       "count  17898.000000       17898.000000    17898.000000  17898.000000  \n",
       "mean      26.326515           8.303556      104.857709      0.091574  \n",
       "std       19.470572           4.506092      106.514540      0.288432  \n",
       "min        7.370432          -3.139270       -1.976976      0.000000  \n",
       "25%       14.437332           5.781506       34.960504      0.000000  \n",
       "50%       18.461316           8.433515       83.064556      0.000000  \n",
       "75%       28.428104          10.702959      139.309330      0.000000  \n",
       "max      110.642211          34.539844     1191.000837      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4ZHzC9laS_k",
    "outputId": "44a9d9d0-23fb-4247-d102-e5b446f520d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17898, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "41bHGH-oaVYr",
    "outputId": "fb85b0f3-3fcb-4e1a-c612-6bc3811e9fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    16259\n",
       "1     1639\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htru2_data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEIJK01YaXIK",
    "outputId": "5b02e140-641b-479d-a2e6-6c4d2658cb3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16108, 8), (16108,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data (80/10/10% - train/validation/test)\n",
    "\n",
    "X = htru2_data.drop(\"class\", axis=1)\n",
    "y = htru2_data[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4Tt7sSwaaDM",
    "outputId": "05bf2451-8c99-4591-c6bf-3812663a2869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14318, 8), (14318,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1/0.9, random_state=SEED)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7m10HLxqa2s8",
    "outputId": "4c7171d7-ad12-4a5c-a6bb-fedc72b22618"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14318, 8), (14318,), (1790, 8), (1790,), (1790, 8), (1790,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU8hUz4la4Yc",
    "outputId": "76a65d08-ebfc-46a7-ba8b-f5495f0b6351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(class\n",
       " 0    13014\n",
       " 1     1304\n",
       " Name: count, dtype: int64,\n",
       " class\n",
       " 0    1627\n",
       " 1     163\n",
       " Name: count, dtype: int64,\n",
       " class\n",
       " 0    1618\n",
       " 1     172\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(), y_val.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-eVMmUye-39"
   },
   "source": [
    "# XGBoost StackingClassifier fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pECwAqsbm1k3",
    "outputId": "f6008d89-641c-4af8-b9dc-364459feae77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9838\n",
      "AUC: 0.9417\n",
      "F1-score: 0.9134\n",
      "Precision: 0.9387\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1608   10]\n",
      " [  19  153]]\n",
      "Runtime: 1267.7948264230508\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9855\n",
      "AUC: 0.9452\n",
      "F1-score: 0.9222\n",
      "Precision: 0.9506\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1610    8]\n",
      " [  18  154]]\n",
      "Runtime: 1091.9435918859672\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9849\n",
      "AUC: 0.9449\n",
      "F1-score: 0.9194\n",
      "Precision: 0.9448\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1609    9]\n",
      " [  18  154]]\n",
      "Runtime: 1099.969440043904\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9849\n",
      "AUC: 0.9449\n",
      "F1-score: 0.9194\n",
      "Precision: 0.9448\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1609    9]\n",
      " [  18  154]]\n",
      "Runtime: 1178.3232512359973\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9849\n",
      "AUC: 0.9423\n",
      "F1-score: 0.9189\n",
      "Precision: 0.9503\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1610    8]\n",
      " [  19  153]]\n",
      "Runtime: 966.5630943127908\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9362\n",
      "Accuracy: 0.9855\n",
      "AUC: 0.9452\n",
      "F1-score: 0.9222\n",
      "Precision: 0.9506\n",
      "Recall score: 0.8953\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9848\n",
      "Average AUC: 0.9438\n",
      "Average f1-score: 0.9187\n",
      "Average precision: 0.9458\n",
      "Average recall score: 0.8930\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # XGBoost parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.3),\n",
    "          f\"max_depth_{j}\" : trial.suggest_int(f\"max_depth_{j}\", 3, 10),\n",
    "          f\"gamma_{j}\" : trial.suggest_uniform(f\"gamma_{j}\", 0, 0.5),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1),\n",
    "          f\"reg_alpha_{j}\" : trial.suggest_loguniform(f\"reg_alpha_{j}\", 1.0e-8, 1),\n",
    "          f\"reg_lambda_{j}\" : trial.suggest_loguniform(f\"reg_lambda_{j}\", 1.8e-8, 10),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 50, 500)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"max_depth\" : params[f\"max_depth_{j}\"],\n",
    "          \"gamma\" : params[f\"gamma_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"reg_alpha\" : params[f\"reg_alpha_{j}\"],\n",
    "          \"reg_lambda\" : params[f\"reg_lambda_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"xgb{j}\", xgb.XGBClassifier(random_state=SEED_OBJ, **model_params)))\n",
    "\n",
    "    # Parameters of LogisticRegression\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_train, y_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  xgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  xgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = xgb_study.best_trial.params\n",
    "\n",
    "  # XGBoost StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"max_depth\" : best_params[f\"max_depth_{k}\"],\n",
    "        \"gamma\" : best_params[f\"gamma_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"reg_alpha\" : best_params[f\"reg_alpha_{k}\"],\n",
    "        \"reg_lambda\" : best_params[f\"reg_lambda_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"xgb{k}\", xgb.XGBClassifier(random_state=SEED, **model_params)))\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned XGBoost StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < xgb_study.best_trial.value:\n",
    "    max_val_auc_score = xgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Best validation auc scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_xgboost_without_smote.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sj2-vWYe-WF"
   },
   "source": [
    "# XGBoost StackingClassifier with SMOTE and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQ8cMqMifMGX",
    "outputId": "3486e338-5a27-4332-b020-7367510b0c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9676\n",
      "AUC: 0.9405\n",
      "F1-score: 0.8432\n",
      "Precision: 0.7879\n",
      "Recall score: 0.9070\n",
      "Confusion matrix:\n",
      " [[1576   42]\n",
      " [  16  156]]\n",
      "Runtime: 1153.9654740320984\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9676\n",
      "AUC: 0.9457\n",
      "F1-score: 0.8449\n",
      "Precision: 0.7822\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1574   44]\n",
      " [  14  158]]\n",
      "Runtime: 1443.682869229\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9654\n",
      "AUC: 0.9419\n",
      "F1-score: 0.8351\n",
      "Precision: 0.7696\n",
      "Recall score: 0.9128\n",
      "Confusion matrix:\n",
      " [[1571   47]\n",
      " [  15  157]]\n",
      "Runtime: 1619.6670232780743\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9648\n",
      "AUC: 0.9442\n",
      "F1-score: 0.8338\n",
      "Precision: 0.7633\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1569   49]\n",
      " [  14  158]]\n",
      "Runtime: 1452.2442924058996\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9659\n",
      "AUC: 0.9448\n",
      "F1-score: 0.8382\n",
      "Precision: 0.7707\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1571   47]\n",
      " [  14  158]]\n",
      "Runtime: 1307.6255185490008\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9487\n",
      "Accuracy: 0.9648\n",
      "AUC: 0.9442\n",
      "F1-score: 0.8338\n",
      "Precision: 0.7633\n",
      "Recall score: 0.9186\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9663\n",
      "Average AUC: 0.9434\n",
      "Average f1-score: 0.8390\n",
      "Average precision: 0.7747\n",
      "Average recall score: 0.9151\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # XGBoost parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.3),\n",
    "          f\"max_depth_{j}\" : trial.suggest_int(f\"max_depth_{j}\", 3, 10),\n",
    "          f\"gamma_{j}\" : trial.suggest_uniform(f\"gamma_{j}\", 0, 0.5),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1),\n",
    "          f\"reg_alpha_{j}\" : trial.suggest_loguniform(f\"reg_alpha_{j}\", 1.0e-8, 1),\n",
    "          f\"reg_lambda_{j}\" : trial.suggest_loguniform(f\"reg_lambda_{j}\", 1.8e-8, 10),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 50, 500)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"max_depth\" : params[f\"max_depth_{j}\"],\n",
    "          \"gamma\" : params[f\"gamma_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"reg_alpha\" : params[f\"reg_alpha_{j}\"],\n",
    "          \"reg_lambda\" : params[f\"reg_lambda_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"xgb{j}\", xgb.XGBClassifier(random_state=SEED_OBJ, **model_params)))\n",
    "\n",
    "    # SMOTE parameter\n",
    "\n",
    "    smote_kn = trial.suggest_int(\"k_neighbors\", 5, 50)\n",
    "\n",
    "    smote_train = SMOTE(sampling_strategy=\"minority\", k_neighbors=smote_kn, random_state=SEED_OBJ)\n",
    "    X_smote_train, y_smote_train = smote_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_smote_train, y_smote_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  xgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  xgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = xgb_study.best_trial.params\n",
    "\n",
    "  # XGBoost StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"max_depth\" : best_params[f\"max_depth_{k}\"],\n",
    "        \"gamma\" : best_params[f\"gamma_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"reg_alpha\" : best_params[f\"reg_alpha_{k}\"],\n",
    "        \"reg_lambda\" : best_params[f\"reg_lambda_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"xgb{k}\", xgb.XGBClassifier(random_state=SEED, **model_params)))\n",
    "\n",
    "  # SMOTE best parameter\n",
    "\n",
    "  smote_kn = best_params[\"k_neighbors\"]\n",
    "\n",
    "  smote_train = SMOTE(sampling_strategy=\"minority\", k_neighbors=smote_kn, random_state=SEED)\n",
    "  X_smote_train, y_smote_train = smote_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned XGBoost StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_smote_train, y_smote_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < xgb_study.best_trial.value:\n",
    "    max_val_auc_score = xgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Best validation auc scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_xgboost_with_smote.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOJ8SEaUfOet"
   },
   "source": [
    "# XGBoost StackingClassifier with Oversampling (ADASYN) and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqGqxJEzfYp8",
    "outputId": "b221b636-1f5e-4fd1-8ae6-2bc745d45bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9547\n",
      "AUC: 0.9360\n",
      "F1-score: 0.7949\n",
      "Precision: 0.7040\n",
      "Recall score: 0.9128\n",
      "Confusion matrix:\n",
      " [[1552   66]\n",
      " [  15  157]]\n",
      "Runtime: 1343.9769560031127\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9503\n",
      "AUC: 0.9361\n",
      "F1-score: 0.7802\n",
      "Precision: 0.6781\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1543   75]\n",
      " [  14  158]]\n",
      "Runtime: 1623.5224057061132\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9514\n",
      "AUC: 0.9341\n",
      "F1-score: 0.7830\n",
      "Precision: 0.6856\n",
      "Recall score: 0.9128\n",
      "Confusion matrix:\n",
      " [[1546   72]\n",
      " [  15  157]]\n",
      "Runtime: 1658.520180603955\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9497\n",
      "AUC: 0.9358\n",
      "F1-score: 0.7783\n",
      "Precision: 0.6752\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1542   76]\n",
      " [  14  158]]\n",
      "Runtime: 1555.994449403137\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9564\n",
      "AUC: 0.9395\n",
      "F1-score: 0.8020\n",
      "Precision: 0.7117\n",
      "Recall score: 0.9186\n",
      "Confusion matrix:\n",
      " [[1554   64]\n",
      " [  14  158]]\n",
      "Runtime: 1847.022601078963\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9441\n",
      "Accuracy: 0.9547\n",
      "AUC: 0.9360\n",
      "F1-score: 0.7949\n",
      "Precision: 0.7040\n",
      "Recall score: 0.9128\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9525\n",
      "Average AUC: 0.9363\n",
      "Average f1-score: 0.7877\n",
      "Average precision: 0.6909\n",
      "Average recall score: 0.9163\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # XGBoost parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.3),\n",
    "          f\"max_depth_{j}\" : trial.suggest_int(f\"max_depth_{j}\", 3, 10),\n",
    "          f\"gamma_{j}\" : trial.suggest_uniform(f\"gamma_{j}\", 0, 0.5),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1),\n",
    "          f\"reg_alpha_{j}\" : trial.suggest_loguniform(f\"reg_alpha_{j}\", 1.0e-8, 1),\n",
    "          f\"reg_lambda_{j}\" : trial.suggest_loguniform(f\"reg_lambda_{j}\", 1.8e-8, 10),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 50, 500)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"max_depth\" : params[f\"max_depth_{j}\"],\n",
    "          \"gamma\" : params[f\"gamma_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"reg_alpha\" : params[f\"reg_alpha_{j}\"],\n",
    "          \"reg_lambda\" : params[f\"reg_lambda_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"xgb{j}\", xgb.XGBClassifier(random_state=SEED_OBJ, **model_params)))\n",
    "\n",
    "    # ADASYN parameter\n",
    "\n",
    "    adasyn_nn = trial.suggest_int(\"n_neighbors\", 5, 50)\n",
    "\n",
    "    adasyn_train = ADASYN(sampling_strategy=\"minority\", n_neighbors=adasyn_nn, random_state=SEED_OBJ)\n",
    "    X_adasyn_train, y_adasyn_train = adasyn_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_adasyn_train, y_adasyn_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  xgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  xgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = xgb_study.best_trial.params\n",
    "\n",
    "  # XGBoost StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"max_depth\" : best_params[f\"max_depth_{k}\"],\n",
    "        \"gamma\" : best_params[f\"gamma_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"reg_alpha\" : best_params[f\"reg_alpha_{k}\"],\n",
    "        \"reg_lambda\" : best_params[f\"reg_lambda_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"xgb{k}\", xgb.XGBClassifier(random_state=SEED, **model_params)))\n",
    "\n",
    "  # ADASYN best parameter\n",
    "\n",
    "  adasyn_nn = best_params[\"n_neighbors\"]\n",
    "\n",
    "  adasyn_train = ADASYN(sampling_strategy=\"minority\", n_neighbors=adasyn_nn, random_state=SEED)\n",
    "  X_adasyn_train, y_adasyn_train = adasyn_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned XGBoost StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_adasyn_train, y_adasyn_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < xgb_study.best_trial.value:\n",
    "    max_val_auc_score = xgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Test scores with best validation AUC score\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_xgboost_with_adasyn.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGAez5YSmOpS"
   },
   "source": [
    "# XGBoost StackingClassifier with Undersampling (TomekLinks) and fine tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rBYU-FnmRtP",
    "outputId": "a71c4fac-aeee-4ab7-9be2-b69a40b9606f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//////////*\n",
      "Trial 1   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9838\n",
      "AUC: 0.9417\n",
      "F1-score: 0.9134\n",
      "Precision: 0.9387\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1608   10]\n",
      " [  19  153]]\n",
      "Runtime: 1071.0207518369425\n",
      "\n",
      "//////////*\n",
      "Trial 2   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9844\n",
      "AUC: 0.9446\n",
      "F1-score: 0.9167\n",
      "Precision: 0.9390\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1608   10]\n",
      " [  18  154]]\n",
      "Runtime: 1095.5807053970639\n",
      "\n",
      "//////////*\n",
      "Trial 3   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9844\n",
      "AUC: 0.9420\n",
      "F1-score: 0.9162\n",
      "Precision: 0.9444\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1609    9]\n",
      " [  19  153]]\n",
      "Runtime: 1056.1910522000398\n",
      "\n",
      "//////////*\n",
      "Trial 4   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9844\n",
      "AUC: 0.9446\n",
      "F1-score: 0.9167\n",
      "Precision: 0.9390\n",
      "Recall score: 0.8953\n",
      "Confusion matrix:\n",
      " [[1608   10]\n",
      " [  18  154]]\n",
      "Runtime: 1119.3994308381807\n",
      "\n",
      "//////////*\n",
      "Trial 5   *\n",
      "//////////*\n",
      "\n",
      "Accuracy: 0.9827\n",
      "AUC: 0.9411\n",
      "F1-score: 0.9080\n",
      "Precision: 0.9273\n",
      "Recall score: 0.8895\n",
      "Confusion matrix:\n",
      " [[1606   12]\n",
      " [  19  153]]\n",
      "Runtime: 1372.4465778740123\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "//////////////////////////////////////////*\n",
      "Scores with the best validation AUC score *\n",
      "//////////////////////////////////////////*\n",
      "\n",
      "Best validation AUC: 0.9383\n",
      "Accuracy: 0.9827\n",
      "AUC: 0.9411\n",
      "F1-score: 0.9080\n",
      "Precision: 0.9273\n",
      "Recall score: 0.8895\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "///////////////*\n",
      "Average scores *\n",
      "///////////////*\n",
      "\n",
      "Average accuracy: 0.9839\n",
      "Average AUC: 0.9428\n",
      "Average f1-score: 0.9142\n",
      "Average precision: 0.9377\n",
      "Average recall score: 0.8919\n"
     ]
    }
   ],
   "source": [
    "max_metrics = []\n",
    "max_val_auc_score = 0\n",
    "avg_accuracy, avg_auc, avg_f1, avg_precision, avg_recall = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(TRIALS):\n",
    "\n",
    "  t1 = time.perf_counter()\n",
    "  SEED = SEED + 1\n",
    "\n",
    "  def objective(trial):\n",
    "\n",
    "    # XGBoost parameters\n",
    "\n",
    "    base_models = []\n",
    "    for j in range(MODEL_NUM):\n",
    "\n",
    "      SEED_OBJ + 1\n",
    "\n",
    "      params = {\n",
    "          f\"learning_rate_{j}\" : trial.suggest_loguniform(f\"learning_rate_{j}\", 0.01, 0.3),\n",
    "          f\"max_depth_{j}\" : trial.suggest_int(f\"max_depth_{j}\", 3, 10),\n",
    "          f\"gamma_{j}\" : trial.suggest_uniform(f\"gamma_{j}\", 0, 0.5),\n",
    "          f\"colsample_bytree_{j}\" : trial.suggest_uniform(f\"colsample_bytree_{j}\", 0.5, 1),\n",
    "          f\"reg_alpha_{j}\" : trial.suggest_loguniform(f\"reg_alpha_{j}\", 1.0e-8, 1),\n",
    "          f\"reg_lambda_{j}\" : trial.suggest_loguniform(f\"reg_lambda_{j}\", 1.8e-8, 10),\n",
    "          f\"subsample_{j}\" : trial.suggest_uniform(f\"subsample_{j}\", 0.5, 1),\n",
    "          f\"n_estimators_{j}\" : trial.suggest_int(f\"n_estimators_{j}\", 50, 500)\n",
    "      }\n",
    "      model_params = {\n",
    "          \"learning_rate\" : params[f\"learning_rate_{j}\"],\n",
    "          \"max_depth\" : params[f\"max_depth_{j}\"],\n",
    "          \"gamma\" : params[f\"gamma_{j}\"],\n",
    "          \"colsample_bytree\" : params[f\"colsample_bytree_{j}\"],\n",
    "          \"reg_alpha\" : params[f\"reg_alpha_{j}\"],\n",
    "          \"reg_lambda\" : params[f\"reg_lambda_{j}\"],\n",
    "          \"subsample\" : params[f\"subsample_{j}\"],\n",
    "          \"n_estimators\" : params[f\"n_estimators_{j}\"]\n",
    "      }\n",
    "      base_models.append((f\"xgb{j}\", xgb.XGBClassifier(random_state=SEED_OBJ, **model_params)))\n",
    "\n",
    "    # TomekLinks\n",
    "\n",
    "    tomek_train = TomekLinks(sampling_strategy=\"majority\")\n",
    "    X_tomek_train, y_tomek_train = tomek_train.fit_resample(X_train, y_train)\n",
    "\n",
    "    # LogisticRegression parameters\n",
    "\n",
    "    lr_C = trial.suggest_loguniform(\"C\", 0.001, 10)\n",
    "    lr_max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n",
    "\n",
    "    stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                  final_estimator=LogisticRegression(random_state=SEED_OBJ, C=lr_C, max_iter=lr_max_iter),\n",
    "                                  cv=kfold,\n",
    "                                  stack_method=\"predict_proba\")\n",
    "\n",
    "\n",
    "    # Fitting the model and evaluating with auc metric\n",
    "\n",
    "    stack_clf.fit(X_tomek_train, y_tomek_train)\n",
    "    y_pred = stack_clf.predict(X_val)\n",
    "\n",
    "    auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "  # Optuna sampler\n",
    "\n",
    "  SAMPLER = TPESampler(seed=SEED)\n",
    "\n",
    "  # Optuna study\n",
    "\n",
    "  xgb_study = optuna.create_study(direction=\"maximize\", sampler=SAMPLER)\n",
    "  xgb_study.optimize(objective, n_trials=NUM_ITER)\n",
    "\n",
    "  # Optuna best parameters\n",
    "\n",
    "  best_params = xgb_study.best_trial.params\n",
    "\n",
    "  # XGBoost StackingClassifier best parameters\n",
    "\n",
    "  base_models = []\n",
    "  for k in range(MODEL_NUM):\n",
    "\n",
    "    model_params = {\n",
    "        \"learning_rate\" : best_params[f\"learning_rate_{k}\"],\n",
    "        \"max_depth\" : best_params[f\"max_depth_{k}\"],\n",
    "        \"gamma\" : best_params[f\"gamma_{k}\"],\n",
    "        \"colsample_bytree\" : best_params[f\"colsample_bytree_{k}\"],\n",
    "        \"reg_alpha\" : best_params[f\"reg_alpha_{k}\"],\n",
    "        \"reg_lambda\" : best_params[f\"reg_lambda_{k}\"],\n",
    "        \"subsample\" : best_params[f\"subsample_{k}\"],\n",
    "        \"n_estimators\" : best_params[f\"n_estimators_{k}\"]\n",
    "    }\n",
    "    base_models.append((f\"xgb{k}\", xgb.XGBClassifier(random_state=SEED, **model_params)))\n",
    "\n",
    "  # TomekLinks\n",
    "\n",
    "  tomek_train = TomekLinks(sampling_strategy=\"majority\")\n",
    "  X_tomek_train, y_tomek_train = tomek_train.fit_resample(X_train, y_train)\n",
    "\n",
    "  # LogisticRegression best parameters\n",
    "\n",
    "  lr_C = best_params[\"C\"]\n",
    "  lr_max_iter = best_params[\"max_iter\"]\n",
    "\n",
    "  # Fine tuned XGBoost StackingClassifier\n",
    "\n",
    "  stack_clf = StackingClassifier(estimators=base_models,\n",
    "                                final_estimator=LogisticRegression(random_state=SEED, C=lr_C, max_iter=lr_max_iter),\n",
    "                                cv=kfold,\n",
    "                                stack_method=\"predict_proba\")\n",
    "\n",
    "  stack_clf.fit(X_tomek_train, y_tomek_train)\n",
    "\n",
    "  y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  auc = roc_auc_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  # Max AUC validation scores\n",
    "\n",
    "  if max_val_auc_score < xgb_study.best_trial.value:\n",
    "    max_val_auc_score = xgb_study.best_trial.value\n",
    "    max_metrics = [accuracy, auc, f1, precision, recall]\n",
    "\n",
    "  # Average scores\n",
    "\n",
    "  avg_accuracy += accuracy\n",
    "  avg_auc += auc\n",
    "  avg_f1 += f1\n",
    "  avg_precision += precision\n",
    "  avg_recall += recall\n",
    "\n",
    "  print(\"\\n\" + 10*\"/\" + \"*\")\n",
    "  print(f\"Trial {i+1}\" + 3*\" \" + \"*\")\n",
    "  print(10*\"/\" + \"*\")\n",
    "\n",
    "  print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "  print(f\"AUC: {auc:.4f}\")\n",
    "  print(f\"F1-score: {f1:.4f}\")\n",
    "  print(f\"Precision: {precision:.4f}\")\n",
    "  print(f\"Recall score: {recall:.4f}\")\n",
    "  print(f\"Confusion matrix:\\n {conf_matrix}\")\n",
    "\n",
    "  t2 = time.perf_counter()\n",
    "  print(\"Runtime:\", t2-t1)\n",
    "\n",
    "# Test scores with best validation AUC score\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 42*\"/\" + \"*\")\n",
    "print(\"Scores with the best validation AUC score *\")\n",
    "print(42*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nBest validation AUC: {(max_val_auc_score):.4f}\")\n",
    "print(f\"Accuracy: {(max_metrics[0]):.4f}\")\n",
    "print(f\"AUC: {(max_metrics[1]):.4f}\")\n",
    "print(f\"F1-score: {(max_metrics[2]):.4f}\")\n",
    "print(f\"Precision: {(max_metrics[3]):.4f}\")\n",
    "print(f\"Recall score: {(max_metrics[4]):.4f}\")\n",
    "\n",
    "# Printing average scores\n",
    "\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "print(\"\\n\" + 15*\"/\" + \"*\")\n",
    "print(\"Average scores *\")\n",
    "print(15*\"/\" + \"*\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {(avg_accuracy / TRIALS):.4f}\")\n",
    "print(f\"Average AUC: {(avg_auc / TRIALS):.4f}\")\n",
    "print(f\"Average f1-score: {(avg_f1 / TRIALS):.4f}\")\n",
    "print(f\"Average precision: {(avg_precision / TRIALS):.4f}\")\n",
    "print(f\"Average recall score: {(avg_recall / TRIALS):.4f}\")\n",
    "\n",
    "# Saving the model\n",
    "\n",
    "pickle.dump(stack_clf, open(\"stacked_xgboost_with_tomeklinks.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfruHYq51O9U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWI3eJuxUGry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF8D0sHjUGfq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
